{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "\\cfrac{1}{Z(t)} = \\exp\\left[\\int_0^t \\Theta (u)dW(u)+\\cfrac{1}{2}\\Theta^2(u)du   \\right]\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "X(t) = \\int_0^t \\Theta (u)dW(u)+\\cfrac{1}{2}\\Theta^2(u)du\n",
    "$$\n",
    "then \n",
    "\\begin{align*}\n",
    "d[\\cfrac{1}{Z(t)}] &= \\cfrac{d}{dt}[e^{X(t)}] \\\\\n",
    "&= e^{X(t)}dX(t) + \\cfrac{1}{2}e^{X(t)}dX(t)X(t) \\\\\n",
    "&= \\cfrac{1}{Z(t)}[\\Theta(t)dW(t)+\\frac{1}{2}\\Theta^2(t)dt]+\\cfrac{1}{2Z(t)}\\Theta^2(t)dt \\\\\n",
    "&= \\cfrac{1}{Z(t)}[\\Theta(t)dW(t) + \\Theta^2(t)dt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "By Lemma 5.2.2, since $\\tilde{M}(t)$ is a martingale, for any $0\\leq s\\leq t$, we have\n",
    "$$\n",
    "E[Z(t)\\tilde{M}(t)|\\mathcal{F}_s] = Z(s)E[\\frac{1}{Z(t)}Z(t)\\tilde{M}(t)|\\mathcal{F}_s] = Z(s)\\tilde{M}(s)\n",
    "$$\n",
    "Hence $Z(t)\\tilde{M}(t)$ is a martingale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "Since $dM(t)=\\Gamma(t)dW(t)$, we have\n",
    "\\begin{align*}\n",
    "d[\\tilde{M}(t)] &= d[M(t)\\frac{1}{Z(t)}] \\\\\n",
    "&= \\frac{1}{Z(t)}dM(t) + M(t)d[\\cfrac{1}{Z(t)}] + d[M(t)]d[\\frac{1}{Z(t)}] \\\\\n",
    "&= \\frac{1}{Z(t)}\\Gamma(t)dW(t) + \\frac{1}{Z(t)}M(t)\\Theta(t)dW(t) + \\frac{1}{Z(t)}[\\Gamma(t)\\Theta(t)+M(t)\\Theta^2(t)M(t)]dt \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)\n",
    "\n",
    "Define $\\tilde{\\Gamma(t)} = \\cfrac{\\Gamma(t)+M(t)\\Theta(t)}{Z(t)}$, then\n",
    "\\begin{align*}\n",
    "d[\\tilde{M}(t)] &= \\cfrac{1}{Z(t)}[\\Gamma(t)+M(t)\\Theta(t)]dW(t) + \\cfrac{\\Theta(t)}{Z(t)}[\\Gamma(t)+M(t)\\Theta(t)M(t)]dt \\\\\n",
    "&= \\tilde{\\Gamma}(t)dW(t) + \\tilde{\\Gamma}(t)\\Theta(t)dt \\\\\n",
    "&= \\tilde{\\Gamma}(t)dW(t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that for any positive $\\tilde{P}$-martingale $M$, $dM_t = M_t \\cdot \\frac{1}{M_t} dM_t$. By Martingale Representation Theorem, $dM_t = \\tilde{\\Gamma}_t d\\tilde{W}_t$ for some adapted process $\\Gamma_t$. So $dM_t = M_t (\\frac{\\tilde{\\Gamma}(t)}{M(t)}) d\\tilde{W}_t$. i.e. any positive martingale must be the exponential of an integral w.r.t. Brownian motion. Taking into account discounting factor and apply ItÃ´'s product rule, we can show every strictly positive asset is a generalized geometric Brownian motion.\n",
    "\n",
    "(i)\n",
    "$V_t D_t = \\tilde{E}[e^{-\\int_{0}^{T} r_u du} V_T | \\mathcal{F}_t] = \\tilde{E}[D_T V_T | \\mathcal{F}_t]$. So $(D_t V_t)_{0 \\leq t \\leq T}$ is a $\\tilde{P}$-martingale. By Martingale Representation Theorem, there exists an adapted process $\\tilde{\\Gamma}_t$, $0 \\leq t \\leq T$, such that $D_t V_t = \\int_{0}^{t} \\tilde{\\Gamma}_s d\\tilde{W}_s$, or equivalently, $V_t = D_t^{-1} \\int_{0}^{t} \\tilde{\\Gamma}_s d\\tilde{W}_s$. Differentiate both sides of the equation, we get $dV_t = R_t D_t^{-1} \\int_{0}^{t} \\tilde{\\Gamma}_s d\\tilde{W}_s dt + D_t^{-1} \\tilde{\\Gamma}_t d\\tilde{W}_t$. i.e. $dV_t = R_t V_t dt + \\frac{\\tilde{\\Gamma}_t}{D_t} dW_t$.\n",
    "\n",
    "(ii)\n",
    "\n",
    "We prove the following more general lemma.\n",
    "\n",
    "Let $X$ be an almost surely positive random variable (i.e. $X > 0$ a.s.) defined on the probability space $(\\Omega, \\mathcal{G}, P)$. Let $\\mathcal{F}$ be a sub-algebra of $\\mathcal{G}$, then $Y = E[X|\\mathcal{F}] > 0$ a.s.\n",
    "\n",
    "By the property of conditional expectation $Y \\geq 0$ a.s. Let $A = \\{Y = 0\\}$, we shall show $P(A) = 0$. Indeed, note $A \\in \\mathcal{F}$, 0 = $E[Y1_A] = E[E[X|\\mathcal{F}]1_A] = E[X1_{A \\cap \\{X \\geq 1\\}}]+E[X1_{A \\cap \\{\\frac{1}{n} \\geq X \\geq \\frac{1}{n+1}\\}}]$ for all $n \\geq 1$. So $P(A \\cap \\{X \\geq 1\\}) = 0$ and $P(A \\cap \\{\\frac{1}{n} \\geq X \\geq \\frac{1}{n+1}\\}) = 0, \\forall n \\geq 1$. This in turn implies $P(A) = 0$.\n",
    "\n",
    "By the above lemma, it is clear that for each $t \\in [0, T]$, $V_t = E[e^{-\\int_{0}^{T} r_u du} V_T | \\mathcal{F}_t] > 0$ a.s.\n",
    "\n",
    "(iii)\n",
    "\n",
    "By (ii), $V_t > 0$ a.s., so $dV_t = V_t\\frac{1}{V_t}dV_t  = V_t \\frac{1}{V_t}(R_t V_t dt + \\frac{\\tilde{\\Gamma}_t}{D_t} d\\tilde{W}_t) = V_t R_t dt + \\sigma_t V_t d\\tilde{W}_t$, where $\\sigma_t = \\frac{\\tilde{\\Gamma}_t}{V_tD_t}$. This shows $V$ follows a generalized geometric Brownian motion."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
